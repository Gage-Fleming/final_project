{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb257109-c575-4498-b5c3-9ec6ad46674d",
   "metadata": {},
   "source": [
    "# 1.0 Library Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8e88d-873d-4af8-a2e0-f7837c0688fd",
   "metadata": {},
   "source": [
    "Below we import the libraries we will be using in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c78937-48d9-495e-b680-6b47855a7ad2",
   "metadata": {},
   "source": [
    "Marker note, references will be made if code was taken from or inspired by a source. The references can be found in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac45e8d-9d11-4770-b052-eb690f12d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow imports.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose\n",
    "\n",
    "# OS import to access files.\n",
    "import os\n",
    "\n",
    "# Numpy Import for array normalizing.\n",
    "import numpy as np\n",
    "\n",
    "# skimage import for photo processing.\n",
    "import skimage as ski\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# matplot import for plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "# xmltodict import is used to parse the annotation file.\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1061219-9e37-4ef9-93b4-5025ae6fee2e",
   "metadata": {},
   "source": [
    "# 2.0 Load Dataset with Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe33a0-05c6-4454-b002-30d006e4a74d",
   "metadata": {},
   "source": [
    "Below we need to load in the dataset with the corresponding annotations. We will also be resizing the dataset to 512x512 images to ensure we have consistency for loading in the neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f8097-36e7-482c-a69d-f6229b218234",
   "metadata": {},
   "source": [
    "## 2.1 Define resize_points function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b859de-03c5-4e1c-bd93-d97b6c131085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_points(polygons, width, height):\n",
    "    \"\"\"\n",
    "    Resizes polygon point coordinates to fit a 512x512 image resolution.\n",
    "    \n",
    "    Parameters:\n",
    "        polygons (list of dict): A list of polygon annotations, each containing:\n",
    "            - '@label' (str): The polygon's name (e.g., 'Post', 'Sign').\n",
    "            - '@points' (str): A semicolon-separated string of x,y coordinates.\n",
    "        width (int): The original image width.\n",
    "        height (int): The original image height.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with labels as keys ('Post', 'Sign', etc.) and their corresponding resized points as NumPy arrays. Missing labels are set to None.\n",
    "    \"\"\"\n",
    "    # Create a container and set zero values for potential missing items.\n",
    "    resized_polygons = {'Post': None,\n",
    "                       'Sign': None,\n",
    "                       'Lower Plate': None,\n",
    "                       'Top Plate': None}\n",
    "    \n",
    "    # Loop through each polygon.\n",
    "    for polygon in polygons:\n",
    "        # Run within a try block as some odd exporting polygons exist in the annotation file.\n",
    "        try:\n",
    "            # Create storage for the related points in this polygon.\n",
    "            points = polygon['@points']\n",
    "        \n",
    "            # Get the image scale based on the x and y factors.\n",
    "            scale_x = 512 / width\n",
    "            scale_y = 512 / height\n",
    "        \n",
    "            # Parse the polygon points.\n",
    "            points = points.split(';')\n",
    "            # Split the points and convert to float format. Reference 22 helped with this task.\n",
    "            points = np.array([list(map(float, point.split(','))) for point in points])\n",
    "        \n",
    "            # Scale the points to the new resolution.\n",
    "            scaled_points = []\n",
    "            for point in points:\n",
    "                scaled_points.append([point[0] * scale_x, point[1] * scale_y])\n",
    "        \n",
    "            # Store the scaled points in the container.\n",
    "            resized_polygons[polygon['@label']] = np.array(scaled_points)\n",
    "        except:\n",
    "            print('Invalid polygon provided', polygon, '\\n')\n",
    "    \n",
    "    # Return stored points\n",
    "    return resized_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85678cf-afb2-4c3f-b185-affb0d9da44a",
   "metadata": {},
   "source": [
    "## 2.2 Define get_mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140da60c-c7a0-4fef-a430-558fe4a3d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(points):\n",
    "    # Create mask array.\n",
    "    mask = np.zeros((512, 512), dtype = np.uint8)\n",
    "\n",
    "    for key in points:\n",
    "        polygon = points[key]\n",
    "        # Continue if polygon is None.\n",
    "        if polygon is None:\n",
    "            continue\n",
    "\n",
    "        # Get mask based on polygon.\n",
    "        # Code from references 23 and 25. 25 was instrumental in getting this properly set up.\n",
    "        rr, cc = ski.draw.polygon(polygon[:, 1], polygon[:, 0], mask.shape)\n",
    "    \n",
    "        # # Overlay mask with white pixels.\n",
    "        mask[rr, cc] = 255\n",
    "\n",
    "    # Return the mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818939c4-85ac-4acd-8e9a-2e13c3cb5702",
   "metadata": {},
   "source": [
    "## 2.3 Load and Resize the Images/Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ea461-6348-4a52-943d-f92be60c7070",
   "metadata": {},
   "source": [
    "Below we will load in the images, resize them and load in and reszie the associated masks. This essentially sets up the dataset for use in machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc76ba-c925-48f7-9c11-cd43645aa764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a storage container.\n",
    "dataset = {}\n",
    "\n",
    "# Parse the XML file\n",
    "# Code reference: [15]\n",
    "with open(\"./object_detection/annotations.xml\", \"r\") as f:\n",
    "    # Import the annotations in a dictionary format.\n",
    "    annotations = xmltodict.parse(f.read())\n",
    "    \n",
    "# Loop through every image object in the annotations and store the image.\n",
    "for image in annotations[\"annotations\"][\"image\"]:\n",
    "    # Get image name.\n",
    "    image_name = image[\"@name\"]\n",
    "    \n",
    "    # Load the image with scikit learn.\n",
    "    photo = ski.io.imread(\"./object_detection/annotated_images\" + '/' + image_name)\n",
    "\n",
    "    # Greyscale the photo to reduce the complexity of the training data. This could lower accuracy, but as the literature review indicates, it seems to be the industry standard.\n",
    "    photo = ski.color.rgb2gray(photo)\n",
    "\n",
    "    # Create a resized image (512x512) in the storage container.\n",
    "    # We'll need to store the images in 512x512 to standardize them for the neural networks\n",
    "    resized_image = ski.transform.resize(photo, (512, 512), anti_aliasing=True)\n",
    "\n",
    "    # Resize and standardize the polygon points\n",
    "    resized_points = resize_points(image['polygon'], photo.shape[1], photo.shape[0])\n",
    "\n",
    "    # Get the mask.\n",
    "    mask = get_mask(resized_points)\n",
    "    \n",
    "    # Store the resized image and all associated masks.\n",
    "    dataset[image_name] = {'image': resized_image, 'mask': mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8cc33-9412-4b9f-ac73-62a158e281e2",
   "metadata": {},
   "source": [
    "## 2.4 Confirm Masks Conform to Resized Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f45e9c22-c841-44f5-997c-6ddef34d8d40",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'f_1723741777.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Choose a random image\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m image_info \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_1723741777.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load photos.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m photo \u001b[38;5;241m=\u001b[39m image_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'f_1723741777.jpg'"
     ]
    }
   ],
   "source": [
    "# Choose a random image\n",
    "image_info = dataset['f_1723741777.jpg']\n",
    "\n",
    "# Load photos.\n",
    "photo = image_info['image']\n",
    "photo_mask = photo.copy()\n",
    "\n",
    "# Layer masks over photo.\n",
    "mask = image_info['mask']\n",
    "\n",
    "# Create plot.\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "# Show base image\n",
    "axes[0].imshow(photo, cmap='gray')\n",
    "axes[0].set_title(\"Base Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show image with mask.\n",
    "axes[1].imshow(mask, cmap='gray')\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Show plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd047cb-4824-4346-a9e8-47e95c783fa5",
   "metadata": {},
   "source": [
    "Above we can see the mask does align with the corresponding image and highlights the items we want to detect in a pixel-wise manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4cc6b-9873-4405-9695-a6ae073ddbd8",
   "metadata": {},
   "source": [
    "## 2.5 Convert Data into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e71766-e588-4f21-9f1d-519de6a6ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image):\n",
    "    if input_image is None:\n",
    "        return None\n",
    "        \n",
    "    # Normalize the pixel range values between [0:1]\n",
    "    image = tf.cast(input_image, dtype = tf.float32) / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff277243-061e-4efa-a36b-ed9834b91dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    normalized_dataset = {}\n",
    "    for key, values in dataset.items():\n",
    "        # Store the normalized data.\n",
    "        normalized_dataset[key] = {'image': normalize(values['image']), \n",
    "                                  'post_mask': normalize(values['post_mask']),\n",
    "                                  'sign_mask': normalize(values['sign_mask']),\n",
    "                                  'lower_plate_mask': normalize(values['lower_plate_mask']),\n",
    "                                  'top_plate_mask': normalize(values['top_plate_mask'])}\n",
    "\n",
    "    return normalized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abae229c-4bba-49cc-b719-cccf9fef89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_data = normalize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef53f7a-559c-468d-ac4a-44fecf511751",
   "metadata": {},
   "source": [
    "# 3.0 Set up Base Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7ecdb-b63e-4882-ba93-83eebe1b8d7e",
   "metadata": {},
   "source": [
    "Below we will set up the base model and run it individually on the different masks.\n",
    "Most of the code below was heavily inspired or taken from reference 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74938d37-e3a7-458e-bf4f-976990c12b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = len(normalized_train_data)\n",
    "batch_size = 32\n",
    "buffer_size = 1000\n",
    "steps_per_epoch = train_length // batch_size\n",
    "width = 512\n",
    "height = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73db424-0428-411d-b0db-579ae2e7fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train'].map(load_train_ds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    " \n",
    "train_ds = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test.batch(BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
